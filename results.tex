\subsection{Value functions in belief wealth space}\label{ssec:val-func}

First, we look at value functions found by using different utility functions in~\autoref{fig:val-func}.
As described in~\autoref{sec:methods} augmented state space encodes the wealth and the belief of the agent. 
Additionally, in our case, time can be derived from wealth exactly by $\text{time} = \frac{\text{wealth}}{\text{observation\ cost}}$.

In the first row, value functions found by the risk-neutral utility (identity function) and an exponential utility show that the induced behaviour is independent of time and risk is only modeled by the belief threshold the agent decides to sell at.
% TODO do we need a policy plot to make this clear?
Exponential utility sets this threshold with the $\lambda$ parameter, which is displayed above the plot.
Positive $\lambda$ models risk-seeking behavior, which results in selling with less belief.

In the second row, we see that the sinh utility function is able to model time dependent behaviour, e.g. selling at a fixed time step. We have parametrized sinh by the scaling and shifting of its input:

\begin{align*}
    U_\text{sinh}(v) := e^{v_1} - e^{-v_2} \text{, where\ } v_i = \text{scale_i}\cdot(v+\text{shift_i})
\end{align*}

First plot shows what we call a fixed time policy, where the agent sells at a specific wealth value.
Second plot shows a similar policy but with less abrupt change in belief threshold.

Lastly, we present the dynamic utility function where $\lambda_i$ is selected from a logarithmically spaced sequence. The sequence range is displayed above each plot. Dynamic utility function was very hard to configure because most picks for lambdas make it unusable. Furthermore, it is very hard to do a grid search because it has so many free parameters (i.e. number of lambdas, or number of time steps.)

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{img/exp_policy.pdf}\\
    \includegraphics[width=0.99\linewidth]{img/sinh_policy.pdf}\\
    \includegraphics[width=0.99\linewidth]{img/dyn_policy.pdf}
    \caption{Value functions exhibiting different risk-behaviors; from top left: risk neutral agent (utility function is the identity function), risk-seeking agent with exponential utility function, two fixed time agents with different time thresholds, two agents with dynamic exponential utility function.}\label{fig:val-func}
\end{figure}

\subsection{From human behavior to utility functions}\label{ssec:human-behavior}

In~\autoref{ssec:val-func}, we have presented how different utility functions result in different policies.
However, our main goal is to derive a utility function and the corresponding risk parameters from behavioral data.
In literature, this is called \keyword{inverse reinforcement learning} \cite{TODO}.

Inverse reinforcement learning is challenging, thus, we only try a naive approach, which can be summarized in three steps: 1) observe behaviour, 2) estimate policy, and 3) derive utility functions and risk parameters using a grid-search.
This approach is plotted in~\autoref{fig:svm_vs_value}, and explained in following. 
First, we get behavioral data for a subject from the expert-setting of the experiment.
For each time step (i.e. year) of each trial we have a data point with the action subject took, the belief shown to the user and the year. These are plotted in~\autoref{fig:svm_vs_value} with blue for wait action, and orange for sell action.

Second, we estimate the policy using a SVM \cite{TODO} with cross-validation. SVM tries to fit a decision boundary, which minimizes the wait actions above and sell actions below this boundary. Here we only take the last two actions in the account, i.e. one waiting and one selling actions (plotted with fuller circles), the rest of the actions are plotted in background.

The last step is to perform a grid search over utility functions and their parameters.
Each utility function and its parameters results in a value function from which a policy can be derived.
The goal of the grid search is to find the policy that is closest to the SVM decision boundary.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{img/fit}
    \caption{Examples from three human behaviors distinctly observed, and reproduced with RL agents. Behaviours by row: 1) Constant belief threshold, modelled by exponential utility, 2) Fixed time threshold, modelled by sinh utility, 3) Mixed strategy, modelled by dynamic exponential utility. Left column shows empirical optimal split of data using cross validation and linear kernel support vector machines, right column shows optimal policy found via grid-search.}
    \label{fig:svm_vs_value}
\end{figure}

\subsection{Analysis of agents}
After reproducing the human behavior with agents we can compare the performance of different agents, which is presented in~\autoref{fig:perf}.
First, the average accumulated reward of each agent is shown in the left box plot, and the ratio of sells in good state is shown on the right bar plot.

Average accumulated reward is depicted by the green line, and the varianve
* Explain boxplot

We observe couple of interesting things.
First, the risk-averse agent has lower variance both on average reward, and ratio of selling in good state. However it has similar number of outliers in average reward to risk-neutral agent, which can be easily explained by the high selling threshold that results in longer waiting time.

Second, even 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{img/performance.pdf}
    \caption{Average reward and ratio of selling in the good state.}\label{fig:perf}
\end{figure}
